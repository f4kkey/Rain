{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(features, data):\n",
    "    choose = np.array([True] * len(data))\n",
    "    for feature in features:\n",
    "        z_score = stats.zscore(data[feature])\n",
    "        z_score = np.abs(z_score) < 3\n",
    "        choose = choose & z_score\n",
    "    data_clean = data[choose]\n",
    "    return data_clean\n",
    "\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df = df.sort_values(by=['row', 'col'], ascending=[True, True]).reset_index(drop=True)\n",
    "\n",
    "df['AWS2'] = df['AWS']\n",
    "\n",
    "features = [\n",
    "    'AWS2', 'CAPE', 'V850', 'EWSS', 'KX', 'U250', 'U850', 'CIN', 'V250', 'R250',\n",
    "    'hour_sin','hour_cos','doy_sin','doy_cos'\n",
    "]\n",
    "target = 'AWS'\n",
    "\n",
    "df = clean(features[0:10],df)\n",
    "\n",
    "df['year'] = df['datetime'].dt.year\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['doy']  = df['datetime'].dt.dayofyear\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['doy_sin']  = np.sin(2 * np.pi * df['doy']  / 365)\n",
    "df['doy_cos']  = np.cos(2 * np.pi * df['doy']  / 365)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_time = pd.Timestamp('2020-10-15 23:00:00')\n",
    "df_train = df[df['datetime'] <= split_time].reset_index(drop=True)\n",
    "df_test  = df[df['datetime'] >  split_time].reset_index(drop=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_train[features] = scaler.fit_transform(df_train[features])\n",
    "df_test[features]  = scaler.transform(df_test[features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences(data, feats, tgt, window_size, horizon):\n",
    "    X, y = [], []\n",
    "    arr_f = data[feats].values\n",
    "    arr_t = data[tgt].values\n",
    "    times = data['datetime'].tolist()\n",
    "    for i in range(window_size, len(data) - horizon + 1):\n",
    "        if any((times[j] - times[j-1]).total_seconds() != 3600 \n",
    "               for j in range(i-window_size+1, i+horizon)):\n",
    "            continue\n",
    "        X.append(arr_f[i-window_size:i])\n",
    "        y.append(arr_t[i:i+horizon])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "window_size = 1\n",
    "horizon     = 6\n",
    "\n",
    "X_train, y_train = make_sequences(df_train, features, target, window_size, horizon)\n",
    "X_test,  y_test  = make_sequences(df_test,  features, target, window_size, horizon)\n",
    "\n",
    "print(\"Train shapes:\", X_train.shape, y_train.shape)\n",
    "print(\"Test  shapes:\", X_test.shape,  y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "\n",
    "\n",
    "n_features = len(features)\n",
    "\n",
    "def build_stacked_lstm(window_size, n_features, horizon,\n",
    "                       units1, units2, dropout1, dropout2, lr):\n",
    "    model = Sequential()\n",
    "    # LSTM layer 1\n",
    "    model.add(LSTM(units1,\n",
    "                   return_sequences=True,\n",
    "                   input_shape=(window_size, n_features)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout1))\n",
    "    # LSTM layer 2\n",
    "    model.add(LSTM(units2, return_sequences=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout2))\n",
    "    # Dense output\n",
    "    model.add(Dense(horizon, activation='linear'))\n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=lr),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_hp(hp):\n",
    "    return build_stacked_lstm(\n",
    "        window_size=window_size,\n",
    "        n_features=n_features,\n",
    "        horizon=horizon,\n",
    "        units1=hp.Choice('units1', [64, 128, 256]),\n",
    "        units2=hp.Choice('units2', [32, 64, 128]),\n",
    "        dropout1=hp.Float('dropout1', 0.1, 0.5, step=0.1),\n",
    "        dropout2=hp.Float('dropout2', 0.1, 0.5, step=0.1),\n",
    "        lr=hp.Choice('lr', [1e-2, 1e-3, 1e-4])\n",
    "    )\n",
    "\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model_hp,\n",
    "    objective='val_loss',    \n",
    "    max_trials=20,             \n",
    "    executions_per_trial=1,\n",
    "    directory='rs_lstm',\n",
    "    project_name='lstm_rs'\n",
    ")\n",
    "\n",
    "\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "best_hp    = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(f\"  units1   = {best_hp.get('units1')}\")\n",
    "print(f\"  units2   = {best_hp.get('units2')}\")\n",
    "print(f\"  dropout1 = {best_hp.get('dropout1'):.2f}\")\n",
    "print(f\"  dropout2 = {best_hp.get('dropout2'):.2f}\")\n",
    "print(f\"  lr       = {best_hp.get('lr')}\")\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(history.history['loss'],  label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Best Model Training')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save('best_model_2.h5')\n",
    "print(\"Saved best_model to best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "# loaded_model = load_model('best_model.h5', compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse  = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test MSE  : {mse:.4f}\")\n",
    "print(f\"Test RMSE : {rmse:.4f}\")\n",
    "print(f\"Test MAE  : {mae:.4f}\")\n",
    "print(f\"Test R^2  : {r2:.4f}\")\n",
    "\n",
    "y_true_flat = y_test.flatten()\n",
    "y_pred_flat = y_pred.flatten()\n",
    "pearson_r, p_value = pearsonr(y_true_flat, y_pred_flat)\n",
    "print(f\"Overall Pearson R: {pearson_r:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "test_day   = pd.to_datetime('2020-04-15 00:00')\n",
    "prev_times = [test_day - timedelta(hours=i) for i in range(window_size, 0, -1)]\n",
    "\n",
    "\n",
    "coord_cols = ['row', 'col']\n",
    "df2020     = df[df['datetime'].dt.year == 2020].copy()\n",
    "grid       = df2020[coord_cols].drop_duplicates().reset_index(drop=True)\n",
    "nP         = len(grid)\n",
    "pred_map = np.full((horizon, nP), np.nan)\n",
    "act_map  = np.full((horizon, nP), np.nan)\n",
    "\n",
    "for idx, pt in grid.iterrows():\n",
    "    cond   = (df2020['row']==pt['row']) & (df2020['col']==pt['col'])\n",
    "    df_loc = df2020[cond].sort_values('datetime').reset_index(drop=True)\n",
    "    df_win = df_loc[df_loc['datetime'].isin(prev_times)]\n",
    "    if len(df_win) != window_size:\n",
    "        continue\n",
    "    \n",
    "    Xd    = df_win[features].values.reshape(1, window_size, len(features))\n",
    "    y_l   = best_model.predict(Xd).flatten()      \n",
    "    pred_map[:, idx] = y_l\n",
    "    \n",
    "    df_act = df_loc[\n",
    "        (df_loc['datetime'] >= test_day) &\n",
    "        (df_loc['datetime'] <  test_day + timedelta(hours=horizon))\n",
    "    ].sort_values('datetime')\n",
    "    if len(df_act) != horizon:\n",
    "        continue\n",
    "    act_map[:, idx] = df_act[target].values\n",
    "\n",
    "print(f\"Built maps for {nP} points.\")\n",
    "\n",
    "vmin = np.nanmin(act_map)\n",
    "vmax = np.nanmax(act_map)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=horizon, ncols=2, figsize=(10, 4*horizon))\n",
    "for h in range(horizon):\n",
    "    ax1, ax2 = axes[h]\n",
    "    sc1 = ax1.scatter(\n",
    "        grid['col'], grid['row'], c=act_map[h],\n",
    "        s=20, vmin=vmin, vmax=vmax\n",
    "    )\n",
    "    ax1.set_title(f'Actual AWS {h+1}h ahead')\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.set_xlabel('col'); ax1.set_ylabel('row')\n",
    "\n",
    "    sc2 = ax2.scatter(\n",
    "        grid['col'], grid['row'], c=pred_map[h],\n",
    "        s=20, vmin=vmin, vmax=vmax\n",
    "    )\n",
    "    ax2.set_title(f'Predicted AWS {h+1}h ahead')\n",
    "    ax2.invert_yaxis()\n",
    "    ax2.set_xlabel('col'); ax2.set_ylabel('row')\n",
    "\n",
    "fig.colorbar(sc1, ax=axes, orientation='vertical', fraction=0.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
