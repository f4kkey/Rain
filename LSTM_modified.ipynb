{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 1,
>>>>>>> 20eab7fd9a44df0fef6c20603637fa1513500f88
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import thư viện\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data_readed.xlsx\")\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
>>>>>>> 20eab7fd9a44df0fef6c20603637fa1513500f88
    "\n",
    "# → Bộ feature mới\n",
    "features = [\"AWS\",\"CAPE\",\"V850\",\"EWSS\",\"KX\",\"U250\",\"U850\",\"CIN\",\"V250\",\"R250\"]\n",
    "target   = \"AWS\"\n",
    "\n",
    "# Chỉ lấy tháng 4 & 10\n",
    "df = df[df[\"datetime\"].dt.month.isin([4,10])].copy()\n",
    "df[\"year\"] = df[\"datetime\"].dt.year\n",
    "\n",
    "# Xác định cột toạ độ (nếu có)\n",
    "coord_cols = [c for c in [\"lat\",\"lon\",\"row\",\"col\"] if c in df.columns]\n",
    "\n",
    "# Group & aggregate\n",
    "group_cols = [\"datetime\"] + coord_cols\n",
    "df = (\n",
    "    df.groupby(group_cols)[features + [target]]\n",
    "      .mean()\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
<<<<<<< HEAD
    "# Tách train/test\n",
    "df2019 = df[df[\"datetime\"].dt.year==2019].sort_values(\"datetime\").reset_index(drop=True)\n",
    "df2020 = df[df[\"datetime\"].dt.year==2020].sort_values(\"datetime\").reset_index(drop=True)\n"
=======
    "df2019 = df[df['datetime'].dt.year==2019].sort_values('datetime').reset_index(drop=True)\n",
    "df2020 = df[df['datetime'].dt.year==2020].sort_values('datetime').reset_index(drop=True)\n",
    "df2019\n"
>>>>>>> 20eab7fd9a44df0fef6c20603637fa1513500f88
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 6,
>>>>>>> 20eab7fd9a44df0fef6c20603637fa1513500f88
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale chỉ các feature ≠ AWS\n",
    "features_to_scale = [f for f in features if f!=\"AWS\"]\n",
    "scaler = StandardScaler()\n",
    "df2019[features_to_scale] = scaler.fit_transform(df2019[features_to_scale])\n",
    "df2020[features_to_scale] = scaler.transform(df2020[features_to_scale])\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": 7,
>>>>>>> 20eab7fd9a44df0fef6c20603637fa1513500f88
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences(data, feats, tgt, window_size, horizon):\n",
    "    X, y = [], []\n",
    "    arr_f = data[feats].values\n",
    "    arr_t = data[tgt].values\n",
    "    for i in range(window_size, len(data)-horizon+1):\n",
    "        X.append(arr_f[i-window_size:i])\n",
    "        y.append(arr_t[i:i+horizon])\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes per-location: (300057, 1, 11) (300057, 6, 2)\n"
     ]
    }
   ],
   "source": [
    "window_size = 1\n",
    "horizon     = 6\n",
    "\n",
    "all_X, all_y = [], []\n",
    "for _, grp in df2019.groupby(coord_cols):\n",
    "    grp = grp.sort_values(\"datetime\").reset_index(drop=True)\n",
    "    Xg, yg = make_sequences(grp, features, target, window_size, horizon)\n",
    "    all_X.append(Xg)\n",
    "    all_y.append(yg)\n",
    "\n",
    "X_train = np.vstack(all_X)\n",
    "y_train = np.vstack(all_y)\n",
    "print(\"Train shapes per-location:\", X_train.shape, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm():\n",
    "    m = Sequential([\n",
    "        Bidirectional(LSTM(64), input_shape=(window_size, len(features))),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        Dense(horizon)\n",
    "    ])\n",
    "    m.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Sanity check (có thể bỏ sau khi chạy ổn)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m X_tr\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m X_tr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m window_size\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m X_tr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(features)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# --- LSTM ---\u001b[39;00m\n\u001b[0;32m     15\u001b[0m model_l \u001b[38;5;241m=\u001b[39m build_lstm()\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cell 6: Cross-Validation (Bi-LSTM + XGBoost)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "lstm_scores, xgb_scores = [], []\n",
    "\n",
    "for tr, vl in tscv.split(X_train):\n",
    "    # 1) Lấy fold\n",
    "    X_tr, X_vl = X_train[tr], X_train[vl]    # already (batch, window_size, n_feats)\n",
    "    y_tr, y_vl = y_train[tr], y_train[vl]\n",
    "\n",
    "    # Sanity check (có thể bỏ sau khi chạy ổn)\n",
    "    assert X_tr.ndim == 3 and X_tr.shape[1] == window_size\n",
    "    assert X_tr.shape[2] == len(features)\n",
    "\n",
    "    # --- LSTM ---\n",
    "    model_l = build_lstm()\n",
    "    es = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "    model_l.fit(\n",
    "        X_tr, y_tr,\n",
    "        validation_data=(X_vl, y_vl),\n",
    "        epochs=100, batch_size=32,\n",
    "        callbacks=[es], verbose=0\n",
    "    )\n",
    "    p_l = model_l.predict(X_vl)\n",
    "    lstm_scores.append(np.sqrt(mean_squared_error(y_vl, p_l)))\n",
    "\n",
    "    # --- XGBoost ---\n",
    "    X_tr_f = X_tr.reshape(len(X_tr), -1)\n",
    "    X_vl_f = X_vl.reshape(len(X_vl), -1)\n",
    "    model_x = xgb.XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        n_estimators=300, max_depth=5,\n",
    "        learning_rate=0.05, subsample=0.8\n",
    "    )\n",
    "    model_x.fit(X_tr_f, y_tr, verbose=False)\n",
    "    p_x = model_x.predict(X_vl_f)\n",
    "    xgb_scores.append(np.sqrt(mean_squared_error(y_vl, p_x)))\n",
    "\n",
    "# Tính CV RMSE và weights\n",
    "l_rmse = np.mean(lstm_scores)\n",
    "x_rmse = np.mean(xgb_scores)\n",
    "w_l = 1 / l_rmse\n",
    "w_x = 1 / x_rmse\n",
    "\n",
    "print(f\"LSTM CV RMSE = {l_rmse:.4f}\")\n",
    "print(f\"XGB  CV RMSE = {x_rmse:.4f}\")\n",
    "print(f\"Ensemble weights → LSTM: {w_l:.2f}, XGB: {w_x:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_final = build_lstm()\n",
    "es = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "lstm_final.fit(X_train, y_train,\n",
    "               validation_split=0.2,\n",
    "               epochs=200, batch_size=32,\n",
    "               callbacks=[es], verbose=1)\n",
    "\n",
    "Xf = X_train.reshape(len(X_train), -1)\n",
    "xgb_final = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    n_estimators=300, max_depth=5,\n",
    "    learning_rate=0.05, subsample=0.8\n",
    ")\n",
    "xgb_final.fit(Xf, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble:\n",
    "    def __init__(self, lstm_model, xgb_model, w_l, w_x):\n",
    "        self.lstm = lstm_model\n",
    "        self.xgb  = xgb_model\n",
    "        self.w_l  = w_l\n",
    "        self.w_x  = w_x\n",
    "\n",
    "    def predict(self, X):\n",
    "        y1 = self.lstm.predict(X)\n",
    "        flat = X.reshape(len(X), -1)\n",
    "        y2 = self.xgb.predict(flat).reshape(y1.shape)\n",
    "        return (self.w_l*y1 + self.w_x*y2)/(self.w_l+self.w_x)\n",
    "\n",
    "ensemble = Ensemble(lstm_final, xgb_final, w_l, w_x)\n",
    "with open(\"ensemble_model_2.pkl\",\"wb\") as f:\n",
    "    pickle.dump(ensemble, f)\n",
    "print(\"Saved ensemble_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# ens = pickle.load(open('ensemble_final.pkl','rb'))\n",
    "# y_pred = ens.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_day  = pd.to_datetime(\"2020-04-15 00:00\")\n",
    "prev_time = [test_day - timedelta(hours=1)]\n",
    "\n",
    "grid = df2020[coord_cols].drop_duplicates().reset_index(drop=True)\n",
    "nP   = len(grid)\n",
    "\n",
    "pred_map = np.full((horizon, nP), np.nan)\n",
    "act_map  = np.full((horizon, nP), np.nan)\n",
    "\n",
    "for idx, pt in grid.iterrows():\n",
    "    cond = np.ones(len(df2020), bool)\n",
    "    for c in coord_cols:\n",
    "        cond &= (df2020[c]==pt[c])\n",
    "    grp = df2020[cond].set_index(\"datetime\").asfreq(\"h\")\n",
    "    cols = features + [target]\n",
    "    grp[cols] = grp[cols].interpolate(\"time\").ffill().bfill()\n",
    "    grp = grp.reset_index().sort_values(\"datetime\")\n",
    "\n",
    "    # 1h input\n",
    "    df_win = grp[grp[\"datetime\"].isin(prev_time)]\n",
    "    if len(df_win)!=1: continue\n",
    "    Xd = df_win[features].values.reshape(1,1,len(features))\n",
    "    pred_map[:,idx] = ensemble.predict(Xd).flatten()\n",
    "\n",
    "    # actual 6h\n",
    "    df_act = grp[\n",
    "        (grp[\"datetime\"]>= test_day)&\n",
    "        (grp[\"datetime\"]<  test_day+timedelta(hours=horizon))\n",
    "    ]\n",
    "    if len(df_act)!=horizon: continue\n",
    "    act_map[:,idx] = df_act[target].values\n",
    "\n",
    "print(f\"Built maps for {nP} points.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin,vmax = np.nanmin(act_map), np.nanmax(act_map)\n",
    "fig, axes = plt.subplots(horizon,2,figsize=(10,4*horizon))\n",
    "for h in range(horizon):\n",
    "    ax1,ax2 = axes[h]\n",
    "    ax1.scatter(grid[coord_cols[1]], grid[coord_cols[0]], c=act_map[h],\n",
    "                cmap='viridis', vmin=vmin, vmax=vmax, s=20)\n",
    "    ax1.set_title(f\"Actual {h+1}h\")\n",
    "    ax2.scatter(grid[coord_cols[1]], grid[coord_cols[0]], c=pred_map[h],\n",
    "                cmap='viridis', vmin=vmin, vmax=vmax, s=20)\n",
    "    ax2.set_title(f\"Predicted {h+1}h\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Metrics & line chart Mean Actual vs Mean Predicted\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "mask  = ~np.isnan(act_map)&~np.isnan(pred_map)\n",
    "y_t,y_p = act_map[mask], pred_map[mask]\n",
    "\n",
    "mse   = mean_squared_error(y_t,y_p)\n",
    "rmse  = np.sqrt(mse)\n",
    "mae   = mean_absolute_error(y_t,y_p)\n",
    "r2    = r2_score(y_t,y_p)\n",
    "pear, _ = pearsonr(y_t,y_p)\n",
    "\n",
    "print(\"Test Day\", test_day.date(), \"Metrics:\")\n",
    "print(f\"  MSE       = {mse:.4f}\")\n",
    "print(f\"  RMSE      = {rmse:.4f}\")\n",
    "print(f\"  MAE       = {mae:.4f}\")\n",
    "print(f\"  R²        = {r2:.4f}\")\n",
    "print(f\"  Pearson R = {pear:.4f}\")\n",
    "\n",
    "# Line chart\n",
    "mean_act = np.nanmean(act_map,axis=1)\n",
    "mean_prd = np.nanmean(pred_map,axis=1)\n",
    "hrs      = np.arange(1,horizon+1)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(hrs, mean_act, marker='o', label='Mean Actual')\n",
    "plt.plot(hrs, mean_prd, marker='x', label='Mean Predicted')\n",
    "plt.xlabel(\"Hour Ahead\"); plt.ylabel(\"AWS\")\n",
    "plt.title(\"Mean Actual vs Mean Predicted\")\n",
    "plt.legend(); plt.grid(True); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
